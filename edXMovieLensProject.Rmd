---
title: "edX MovieLens Project"
author: "James Cruikshanks"
date: "7/17/2021"
output: pdf_document
---

```{r setup, include=FALSE}
load("~/R/edxCapstone/recommender.RData") #make sure to save the workspace image before running RMD.

#Replace with entire script before submission
library(tidyverse)
library(lubridate)
library(knitr)
```

## Introduction

Using a large database of movie ratings, this project builds a machine learning model that predicts how a user will rate a given movie. The dataset includes 10 million ratings of more than 10,000 movies by over 72,000 users and can be downloaded from https://grouplens.org/datasets/movielens/10m/. Each rating contains 6 pieces of information: the user ID, the movie ID and title, the timestamp of the rating, and the genre(s) of the film. We can see the data structure by looking at the first few rows:

```{r raw data, echo=FALSE, warning=FALSE}
kable(head(edx))
```

Using the features present in the dataset, a model is created which is able to approximate and adjust for the effect of each one. How well each movie is liked by all users, how picky each user appears to be by their movie ratings, how well rated each genre appears to be, and even the week a movie was rated all present predictable patterns. These patterns are explored and exploited to predict how any user will rate any movie. The model performance is evaluated by the root mean square error between the predicted ratings and the actual ratings of a partitioned validation dataset, as determined by the following function:


```{r RMSE function, echo=TRUE}
RMSE <- function(actual_ratings, predicted_ratings){
  sqrt(mean((actual_ratings - predicted_ratings) ^ 2))
}
```

## Analysis

Firstly, the dataset is partitioned into a 'train' set and a 'test' set for various models to be tested while avoiding overfitting of the data to the validation set. `r nrow(train_set)` randomly selected ratings make up the training set and the remaining `r nrow(test_set)` ratings are used for model evaluation.

The prediction of a given user's rating of a given movie might be simply estimated by taking the average of all observed movie ratings. Expanding on this, we take the average of each movie's rating across all users, and each users rating across all movies. Called 'The User + Movie Effect Model', it is calculated as follows:

```{r user+movie effect model, echo=TRUE, message=FALSE, warning=FALSE}
#calculate the average rating across all movies and users
mean <- mean(train_set$rating)

#calculate and average user rating above or below the mean, per movie (the 'movie effect')
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(movie_eff = mean(rating - mean))

#calculate average movie rating above or below the mean, per user (the 'user effect')
user_avgs <- train_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  group_by(userId) %>%
  summarize(user_eff = mean(rating - mean - movie_eff))

#predict ratings using user and movie effects model
prediction <- test_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  mutate(pred = mean + user_eff + movie_eff) %>%
  .$pred
```

This model takes into account the effect of the movie, as well as the user. Using this model a RMSE of `r user_movie_RMSE` is achieved

Next, the average of all ratings in a given week is calculated across all of the given time stamps and plotted.

```{r date effect plot, echo=FALSE, fig.height=3, message=FALSE, warning=FALSE}
date_plot
```

The plot shows a trend that can then be integrated into a new model called 'The User + Movie + Date Effect Model' as follows:
```{r date effect model, echo=TRUE, message=FALSE, warning=FALSE}
#approximate linear model to calculate the effect of date on user, movie rating
date_avgs <- train_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  
  #add column with week of rating
  mutate(date = as_datetime(timestamp), week = round_date(date, unit = "week")) %>%
  
  #calculate average movie rating above or below the mean for each user and movie, by week
  group_by(week) %>%
  summarize(date_eff = mean(rating-mean-user_eff-movie_eff))

##predict ratings using user, movie, and date effects model
#calculate week from test set timestamps
test_set_date <- test_set %>%
  mutate(week = round_date(as_datetime(timestamp), unit = "week"))

#calculate predicted rating
prediction <- test_set_date %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(date_avgs, by = 'week') %>%
  mutate(pred = mean + user_eff + movie_eff + date_eff) %>%
  .$pred
```

Using this model a RMSE of `r user_movie_date_RMSE` is achieved, a slight improvement.

Next, the average rating by genre is calculated and plotted with the standard error for genres with over 50000 ratings.

```{r genre effect plot, echo=FALSE, message=FALSE, warning=FALSE}
genre_plot
```

There is variation evident that may be modeled by calculating the average difference for each group of genres, after accounting for the other three effects modeled previously. The model is calculated as follows:

```{r genre effect model, echo=TRUE, message=FALSE, warning=FALSE}
#calculate average impact of genre on user rating of movie
genre_avgs <- train_set %>%
  
  #add column with week of rating
  mutate(week = round_date(as_datetime(timestamp), unit = "week")) %>%
  
  #add user, movie, and date effects
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(date_avgs,by = 'week') %>%
  
  #calculate average movie rating above or below the mean for each user and movie, by genre
  group_by(genres) %>%
  summarize(genre_eff = mean(rating-mean-user_eff-movie_eff-date_eff))

#calculate predicted rating
prediction <- test_set_date %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(date_avgs, by = 'week') %>%
  left_join(genre_avgs, by = 'genres') %>%
  mutate(pred = mean + user_eff + movie_eff + date_eff + genre_eff) %>%
  .$pred
```

'The User + Movie + Date + Genre Model' achieves a RMSE of `r user_movie_date_genre_RMSE`, another improvement. Since the available features have been considered in a simple and consistent manner, we move to investigating the predicted results.

The first insight is that there are many movies and users with small sample sizes.

```{r frequency of ratings, echo=FALSE, fig.height=3, fig.width=3, message=FALSE, warning=FALSE}
movie_count_plot
user_count_plot
```

These can lead to high or low predictions based on small sample sizes, when a more conservative estimate closer to the mean rating of all movies would be preferable. To achieve this outcome in the model we use regularization, which is calculated with a given lambda value as follows:

```{r regularized model, echo=FALSE, message=FALSE, warning=FALSE}
##regularize effects to be conservative when estimating based on small sample sizes
#calculate movie effect with each lambda
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(user_eff = sum(rating - mean)/(n()+l))

#calculate user effect with each lambda
user_avgs <- train_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  group_by(userId) %>%
  summarize(movie_eff = sum(rating - mean - user_eff)/(n()+l))

#calculate date effect with each lambda
date_avgs <- train_set %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(movie_avgs, by = 'movieId') %>%
  mutate(week = round_date(as_datetime(timestamp), unit = "year")) %>%
  group_by(week) %>%
  summarize(date_eff = sum(rating-mean-user_eff-movie_eff)/(n()+l))

#calculate genre effect with each lambda
genre_avgs <- train_set %>% 
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  mutate(week = round_date(as_datetime(timestamp), unit = "year")) %>%
  left_join(date_avgs, by = 'week') %>%
  group_by(genres) %>%
  summarize(genre_eff = sum(rating-mean-user_eff-movie_eff-date_eff)/(n()+l))

#calculate predictions
prediction <- test_set %>%
  mutate(week = round_date(as_datetime(timestamp), unit = "year")) %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(date_avgs, by = 'week') %>%
  left_join(genre_avgs, by = 'genres') %>%
  mutate(pred = mean + user_eff + movie_eff + date_eff + genre_eff) %>%
  .$pred
```

Lambda can be tuned using cross validation. The results of this are plotted below:
```{r lambda plot, echo=FALSE, fig.align = "center", fig.height=3, fig.width=5, message=FALSE, warning=FALSE}
lambda_plot
```

The next insight is that the highest predicted rating is `r max(prediction)` and the lowest is `r min(prediction)`, which is guaranteed to result in a large error on a scale of 0-5. Adding a floor of 0 and a ceiling of 5 to the predictions is sure to improve the model.

## Results

presents the modeling results and discusses the model performance

## Conclusions

a brief summary of the report, its limitations and future work

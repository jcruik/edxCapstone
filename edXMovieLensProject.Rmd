---
title: "edX MovieLens Project"
author: "James Cruikshanks"
date: "7/24/2021"
output: pdf_document
---

```{r setup, include=FALSE}
###edX Capstone Project - Movielens ratings predictor###


###########################################################
# Create edx set, validation set (final hold-out test set)#
###########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(lubridate)
library(knitr)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
# movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                            title = as.character(title),
#                                            genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

#Require and install tidyverse and caret packages
if (!require(tidyverse))
  install.packages("tidyverse")
if (!require(lubridate))
  install.packages("lubridate")
if (!require(caret))
  install.packages("caret")

#load required packages to library
library(tidyverse)
library(lubridate)
library(caret)

##partition the data set created in the provided code into a test set and training set for model training and testing
#set seed for repeatable randomness
set.seed(1, sample.kind = "Rounding")

#create data partition index to split 20% of the edx set into a test set and 80% into a training set
train_index <- createDataPartition(y = edx$rating,
                                   p = 0.8,
                                   list = FALSE)

#create partitioned data sets using index
train_set <- edx[train_index,]
test_set <- edx[-train_index,]

#FOR TESTING ONLY, cut dataset down
# train_set <- train_set %>% slice_sample(n = 100000)
# test_set <- test_set %>% slice_sample(n = 10000)

#remove movies or users which only appear in one set or the other
test_set <- test_set %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

#write function to calculate the RMSE between predicted and actual movie ratings
RMSE <- function(actual_ratings, predicted_ratings){
  sqrt(mean((actual_ratings - predicted_ratings) ^ 2))
}

##Train matrix factorization model
#calculate the average rating across all movies and users
mean <- mean(train_set$rating)

#calculate and average user rating above or below the mean, per movie (the 'movie effect')
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(movie_eff = mean(rating - mean))

#calculate average movie rating above or below the mean, per user (the 'user effect')
user_avgs <- train_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  group_by(userId) %>%
  summarize(user_eff = mean(rating - mean - movie_eff))

#predict ratings using user and movie effects model
prediction <- test_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  mutate(pred = mean + user_eff + movie_eff) %>%
  .$pred

#calculate the RMSE of the predictions
user_movie_RMSE <- RMSE(test_set$rating, prediction)

##add date effect to model
#plot average weekly rating vs. date
date_plot <- train_set %>% 
  mutate(date = as_datetime(timestamp), week = round_date(date, unit = "week")) %>%
  group_by(week) %>%
  summarise(mean = mean(rating)) %>%
  ggplot(aes(week,mean)) +
  geom_point() +
  geom_smooth() +
  ylab("Mean Rating") +
  xlab("Week")

#approximate linear model to calculate the effect of date on user, movie rating
date_avgs <- train_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  
  #add column with week of rating
  mutate(date = as_datetime(timestamp), week = round_date(date, unit = "week")) %>%
  
  #calculate average movie rating above or below the mean for each user and movie, by week
  group_by(week) %>%
  summarize(date_eff = mean(rating-mean-user_eff-movie_eff))

##predict ratings using user, movie, and date effects model
#calculate week from test set timestamps
test_set_date <- test_set %>%
  mutate(week = round_date(as_datetime(timestamp), unit = "week"))

#calculate predicted rating
prediction <- test_set_date %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(date_avgs, by = 'week') %>%
  mutate(pred = mean + user_eff + movie_eff + date_eff) %>%
  .$pred

#calculate the RMSE of the predictions
user_movie_date_RMSE <- RMSE(test_set_date$rating, prediction)

##add genre effect to model
#plot distribution of ratings by genre
genre_plot <- train_set %>%
  group_by(genres) %>%
  filter(n() > 50000) %>%
  summarise(mean = mean(rating), sd = sd(rating), upper = mean + sd, lower = mean - sd) %>%
  ggplot(aes(genres, mean)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  ylab("Mean Rating") +
  xlab("Genre")

#calculate average impact of genre on user rating of movie
genre_avgs <- train_set %>%
  
  #add column with week of rating
  mutate(week = round_date(as_datetime(timestamp), unit = "week")) %>%
  
  #add user, movie, and date effects
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(date_avgs,by = 'week') %>%
  
  #calculate average movie rating above or below the mean for each user and movie, by genre
  group_by(genres) %>%
  summarize(genre_eff = mean(rating-mean-user_eff-movie_eff-date_eff))

#calculate predicted rating
prediction <- test_set_date %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(date_avgs, by = 'week') %>%
  left_join(genre_avgs, by = 'genres') %>%
  mutate(pred = mean + user_eff + movie_eff + date_eff + genre_eff) %>%
  .$pred

#calculate the RMSE of the predictions
user_movie_date_genre_RMSE <- RMSE(test_set_date$rating, prediction)

##plot sample size of each movie, user
#count ratings by movie
movie_count <- train_set %>%
  count(movieId)

#plot the frequency of ratings, by movie
movie_count_plot <- movie_count %>%
  ggplot(aes(n)) +
  geom_histogram(binwidth = 20) +
  ggtitle("Movies") +
  xlab("Number of Ratings") +
  xlim(c(0,1000))

#count ratings by user
user_count <- train_set %>%
  count(userId)

#plot the frequency of ratings, by user
user_count_plot <- user_count %>%
  ggplot(aes(n)) +
  geom_histogram(binwidth = 20) +
  xlab("Number of Ratings") +
  ggtitle("Users") +
  xlim(c(0,1000))

##regularize effects to be conservative when estimating based on small sample sizes
#create an array of lambda values for tuning algorithm
lambdas <- seq(0, 10, 1)

#perform cross validation of the regularized user + movie effects model, different values of lambda
rmses <- sapply(lambdas, function(l){ #supply array of lambda values and run cross validation
  
  #calculate movie effect with each lambda
  movie_avgs <- train_set %>%
    group_by(movieId) %>%
    summarize(user_eff = sum(rating - mean)/(n()+l))
  
  #calculate user effect with each lambda
  user_avgs <- train_set %>%
    left_join(movie_avgs, by = 'movieId') %>%
    group_by(userId) %>%
    summarize(movie_eff = sum(rating - mean - user_eff)/(n()+l))
  
  #calculate date effect with each lambda
  date_avgs <- train_set %>%
    left_join(user_avgs, by = 'userId') %>%
    left_join(movie_avgs, by = 'movieId') %>%
    mutate(week = round_date(as_datetime(timestamp), unit = "week")) %>%
    group_by(week) %>%
    summarize(date_eff = sum(rating-mean-user_eff-movie_eff)/(n()+l))
  
  #calculate genre effect with each lambda
  genre_avgs <- train_set %>% 
    left_join(movie_avgs, by = 'movieId') %>%
    left_join(user_avgs, by = 'userId') %>%
    mutate(week = round_date(as_datetime(timestamp), unit = "week")) %>%
    left_join(date_avgs, by = 'week') %>%
    group_by(genres) %>%
    summarize(genre_eff = sum(rating-mean-user_eff-movie_eff-date_eff)/(n()+l))
  
  #calculate predictions
  prediction <- test_set %>%
    mutate(week = round_date(as_datetime(timestamp), unit = "week")) %>%
    left_join(movie_avgs, by = 'movieId') %>%
    left_join(user_avgs, by = 'userId') %>%
    left_join(date_avgs, by = 'week') %>%
    left_join(genre_avgs, by = 'genres') %>%
    mutate(pred = mean + user_eff + movie_eff + date_eff + genre_eff) %>%
    .$pred
  
  #return RMSE for each lambda
  return(RMSE(prediction, test_set$rating))
})
#plot RMSE against lambda
lambda_plot <- qplot(lambdas,rmses)

#select lambda from cross validation which minimizes RMSE
l <- lambdas[which.min(rmses)]

#record minimum RMSE using regularized model
regularized_RMSE <- min(rmses)

##calculate final model using entire edx data set
#calculate the average rating across all movies and users
mean <- mean(edx$rating)

#calculate movie effect
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(user_eff = sum(rating - mean)/(n()+l))

#calculate user effect
user_avgs <- edx %>%
  left_join(movie_avgs, by = 'movieId') %>%
  group_by(userId) %>%
  summarize(movie_eff = sum(rating - mean - user_eff)/(n()+l))

#calculate date effect
date_avgs <- edx %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(movie_avgs, by = 'movieId') %>%
  mutate(week = round_date(as_datetime(timestamp), unit = "week")) %>%
  group_by(week) %>%
  summarize(date_eff = sum(rating-mean-user_eff-movie_eff)/(n()+l))

#calculate genre effect
genre_avgs <- edx %>% 
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  mutate(week = round_date(as_datetime(timestamp), unit = "week")) %>%
  left_join(date_avgs, by = 'week') %>%
  group_by(genres) %>%
  summarize(genre_eff = sum(rating-mean-user_eff-movie_eff-date_eff)/(n()+l))

#predict ratings for validation set using final model
prediction <- validation %>%
  mutate(week = round_date(as_datetime(timestamp), unit = "week")) %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(date_avgs, by = 'week') %>%
  left_join(genre_avgs, by = 'genres') %>%
  mutate(pred = mean + user_eff + movie_eff + date_eff + genre_eff) %>%
  .$pred

#round predictions which are outside the range of the 0-5 scale
prediction <- pmax(0,prediction)
prediction <- pmin(5,prediction)

#calculate and print RMSE of final model
final_model_RMSE <- RMSE(prediction, validation$rating)

#set the number of digits for rounding
options(digits = 5)


```

## Introduction

Using a large database of movie ratings, this project builds a machine learning model that predicts how a user will rate a given movie. The dataset includes 10 million ratings of more than 10,000 movies by over 72,000 users and can be downloaded from https://grouplens.org/datasets/movielens/10m/. Each rating contains 6 pieces of information: the user ID, the movie ID and title, the timestamp of the rating, and the genre(s) of the film. We can see the data structure by looking at the first few rows:

```{r raw data, echo=FALSE, warning=FALSE}
kable(head(edx))
```

Using the features present in the dataset, a model is created which is able to approximate and adjust for the effect of each one. How well each movie is liked by all users, how picky each user appears to be by their movie ratings, how well rated each genre appears to be, and even the week a movie was rated all present predictable patterns. These patterns are explored and exploited to predict how any user will rate any movie. The model performance is evaluated by the root mean square error between the predicted ratings and the actual ratings of a partitioned validation dataset, as determined by the following function:


```{r RMSE function, echo=TRUE}
RMSE <- function(actual_ratings, predicted_ratings){
  sqrt(mean((actual_ratings - predicted_ratings) ^ 2))
}
```

## Analysis

Firstly, the training dataset 'edx' is partitioned into another 'train' set and a 'test' set for various models to be cross-validated while avoiding overfitting of the data to the 'validation' set. `r nrow(train_set)` randomly selected ratings make up the training set and the remaining `r nrow(test_set)` ratings are used for model evaluation.

The prediction of a given user's rating of a given movie might be simply estimated by taking the average of all observed movie ratings. Expanding on this, we take the average of each movie's rating across all users, and each users rating across all movies. Called 'The User + Movie Effect Model', it is calculated as follows:

```{r user+movie effect model, echo=TRUE, message=FALSE, warning=FALSE}
#calculate the average rating across all movies and users
mean <- mean(train_set$rating)

#calculate and average user rating above or below the mean, per movie (the 'movie effect')
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(movie_eff = mean(rating - mean))

#calculate average movie rating above or below the mean, per user (the 'user effect')
user_avgs <- train_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  group_by(userId) %>%
  summarize(user_eff = mean(rating - mean - movie_eff))

#predict ratings using user and movie effects model
prediction <- test_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  mutate(pred = mean + user_eff + movie_eff) %>%
  .$pred
```

This model takes into account the effect of the movie, as well as the user. Using this model a RMSE of `r user_movie_RMSE` is achieved

Next, the average of all ratings in a given week is calculated across all of the given time stamps and plotted.

```{r date effect plot, echo=FALSE, fig.height=3, message=FALSE, warning=FALSE}
date_plot
```

The plot shows a trend that can then be integrated into a new model called 'The User + Movie + Date Effect Model' as follows:
```{r date effect model, echo=TRUE, message=FALSE, warning=FALSE}
#approximate linear model to calculate the effect of date on user, movie rating
date_avgs <- train_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  
  #add column with week of rating
  mutate(date = as_datetime(timestamp), week = round_date(date, unit = "week")) %>%
  
  #calculate average movie rating above or below the mean for each user and movie, by week
  group_by(week) %>%
  summarize(date_eff = mean(rating-mean-user_eff-movie_eff))

##predict ratings using user, movie, and date effects model
#calculate week from test set timestamps
test_set_date <- test_set %>%
  mutate(week = round_date(as_datetime(timestamp), unit = "week"))

#calculate predicted rating
prediction <- test_set_date %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(date_avgs, by = 'week') %>%
  mutate(pred = mean + user_eff + movie_eff + date_eff) %>%
  .$pred
```

Using this model a RMSE of `r user_movie_date_RMSE` is achieved, a slight improvement.

Next, the average rating by genre is calculated and plotted with the standard error for genres with over 50000 ratings.

```{r genre effect plot, echo=FALSE, message=FALSE, warning=FALSE}
genre_plot
```

There is variation evident that may be modeled by calculating the average difference for each group of genres, after accounting for the other three effects modeled previously. The model is calculated as follows:

```{r genre effect model, echo=TRUE, message=FALSE, warning=FALSE}
#calculate average impact of genre on user rating of movie
genre_avgs <- train_set %>%
  
  #add column with week of rating
  mutate(week = round_date(as_datetime(timestamp), unit = "week")) %>%
  
  #add user, movie, and date effects
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(date_avgs,by = 'week') %>%
  
  #calculate average movie rating above or below the mean for each user and movie, by genre
  group_by(genres) %>%
  summarize(genre_eff = mean(rating-mean-user_eff-movie_eff-date_eff))

#calculate predicted rating
prediction <- test_set_date %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(date_avgs, by = 'week') %>%
  left_join(genre_avgs, by = 'genres') %>%
  mutate(pred = mean + user_eff + movie_eff + date_eff + genre_eff) %>%
  .$pred
```

'The User + Movie + Date + Genre Model' achieves a RMSE of `r user_movie_date_genre_RMSE`, another improvement. Since the available features have been considered in a simple and consistent manner, we move to investigating the predicted results.

The first insight is that most movies and users have small sample sizes.

```{r frequency of ratings, echo=FALSE, fig.height=3, fig.width=3.25, message=FALSE, warning=FALSE}
movie_count_plot
user_count_plot
```
These can lead to overconfident high or low predictions, when a more conservative estimate closer to the mean rating of all movies would be preferable. To model this outcome we regularize each of the four effects, which is calculated as follows:

```{r regularized model, echo=TRUE, message=FALSE, warning=FALSE}
##regularize effects to be conservative when estimating based on small sample sizes
#calculate movie effect with each lambda
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(user_eff = sum(rating - mean)/(n()+l))

#calculate user effect with each lambda
user_avgs <- train_set %>%
  left_join(movie_avgs, by = 'movieId') %>%
  group_by(userId) %>%
  summarize(movie_eff = sum(rating - mean - user_eff)/(n()+l))

#calculate date effect with each lambda
date_avgs <- train_set %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(movie_avgs, by = 'movieId') %>%
  mutate(week = round_date(as_datetime(timestamp), unit = "year")) %>%
  group_by(week) %>%
  summarize(date_eff = sum(rating-mean-user_eff-movie_eff)/(n()+l))

#calculate genre effect with each lambda
genre_avgs <- train_set %>% 
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  mutate(week = round_date(as_datetime(timestamp), unit = "year")) %>%
  left_join(date_avgs, by = 'week') %>%
  group_by(genres) %>%
  summarize(genre_eff = sum(rating-mean-user_eff-movie_eff-date_eff)/(n()+l))

#calculate predictions
prediction <- test_set %>%
  mutate(week = round_date(as_datetime(timestamp), unit = "year")) %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = 'userId') %>%
  left_join(date_avgs, by = 'week') %>%
  left_join(genre_avgs, by = 'genres') %>%
  mutate(pred = mean + user_eff + movie_eff + date_eff + genre_eff) %>%
  .$pred
```

Lambda can be tuned using cross validation. The results of this are plotted below, where the optimal lambda of 5 produces a model with the lowest RMSE, `r regularized_RMSE`:
```{r lambda plot, echo=FALSE, fig.align = "center", fig.height=3, fig.width=5, message=FALSE, warning=FALSE}
lambda_plot
```

The next insight is that the highest predicted rating is `r max(prediction)` and the lowest is `r min(prediction)`, which is guaranteed to result in an error on a scale of 0-5. Adding a floor of 0 and a ceiling of 5 to the predictions is sure to improve the model.

## Results

'The Regularized User + Movie + Date + Genre Model' achieves an RMSE of `r regularized_RMSE` using the training and test sets partitioned for cross-validation. With a consistent and robust model utilizing all the available features of the dataset, this machine learning model is selected as the final model. Next, the model is trained on the full 'edx' training set and tested on the 'validation' set which so far has not been incorporated in any of the analysis.

The results show an RMSE of `r final_model_RMSE` using 'The Final Model'. The additional data improves the model accuracy for the lowest RMSE score seen so far in the analysis. All model results so far are summarized in the table below:

```{r summary table, echo=FALSE, message=FALSE, warning=FALSE}
kable(data.frame(Models=c("The Movie + User Model","The Movie + User + Date Model", "The Movie + User + Date + Genre Model", "The Regularized User + Movie + Date + Genre Model", "The Final Model"),RMSE = c(user_movie_RMSE, user_movie_date_RMSE,user_movie_date_genre_RMSE, regularized_RMSE, final_model_RMSE)))
```


## Conclusions

This report demonstrates the capability of even simple methods using machine learning on large datasets. The MovieLens data contains 10 million observations which were used to predict how any users in the dataset would rate any of the movies. Using linear models of the various features provided in the data led to an algorithm which is capable of predicting, within `r final_model_RMSE`, a users rating from 0-5.

Future work is plentiful. The model could search online databases for much more information on each of the movies rated, finding any number of insights using the properties of each film. The user effect could be expanded by grouping users by similar preferences, potentially increasing the accuracy of ratings by accounting for similar tastes between users. All of this work, and likely much more, has been completed and is available in the public domain for study and reference.
